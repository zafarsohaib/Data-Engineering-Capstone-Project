{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# U.S. Visitors Data-Warehousing \n",
    "## (Data Engineering Capstone Project)\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "The purpose of the data engineering capstone project is to give students a chance to combine what they've learned throughout the program. This project will be an important part of their portfolio that will help them achieve their data engineering-related career goals.\n",
    "\n",
    "The project aims to create a data warehouse for U.S. immigration data in order to understand and easily query for information related to people visiting U.S.\n",
    "\n",
    "### Project Steps\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The plan in this project is to create a Data Warehouse in the cloud, so the Analytics and BI Tools can perform queries related to U.S. immigration for international visitors.\n",
    "\n",
    "#### Data Sets\n",
    "\n",
    " The following data sets are analyzed for this project, povided in the project workspace. The Exploratory Data Analysis (EDA) is performed in the next section.\n",
    "\n",
    "-   **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "-   **World Temperature Data:** This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "-   **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "-   **Airport Code Table:** This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "#### End Solution\n",
    "\n",
    "The end solution would be based on **AWS (Amazon Web Services) Cloud**. In the first phase of the project we would perform **ETL** processes on the given datasets, and stage the data model in **Amazon S3 buckets**. In the second phase of the project, we would load the dimension and fact tables fromt the **AWS S3 buckets** to **AWS Redshift** using the data pipeline created using **Apache Airflow**, and eventually check the data quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![](./images/Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The users could easily query the data warehouse to find information like visitors origin, distribution of visitors by nationality, correlation between the source country and the state destination in U.S., comparison of their regional climate changes, comparision of the demographics of the states visited, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "In the following section, I performed a Exploratory Data Analysis on the data sets provided by Udacity, and discuss what data sets are useful and how we can clean or tranform data into something meaningful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. Immigration Data\n",
    "\n",
    "Form I-94, the Arrival-Departure Record Card, is a form used by U.S. Customs and Border Protection (CBP) intended to keep track of the arrival and departure to/from the United States of people who are not United States citizens or lawful permanent residents. \n",
    "\n",
    "This data comes from the US National Tourism and Trade Office. For each month of 2016 there is a 'sas7bdat' file in this folder **'../../data/18-83510-I94-Data-2016/**. I have used April 2016 for this project, which has 3096313 entries. \n",
    "\n",
    "For the capstone project, the immigration data is treated as the main data set and it will fill the fact table of star schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading Immigration Data\n",
    "immigration_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immigration = pd.read_sas(immigration_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "df_immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking which columns tend to be empty\n",
    "df_immigration.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **cicid**       |  Unique Identifier for the Immigration Data |\n",
    "|   **i94yr**       |   Year |\n",
    "|   **i94mon**      |   Month |\n",
    "|   **i94cit**      |   City Code (Cirty of Origin)|\n",
    "|   **i94res**      |   Country Code (Country of Residence)|\n",
    "|   **i94port**     |   Port City of Entry into U.S.|\n",
    "|   **arrdate**     |   Arrival Date in US (SAS date format)|\n",
    "|   **i94mode**     |   Mode of transportation (1: Air, 2: Sea, 3: Land, 9: Not reported) |\n",
    "|   **i94addr**     |   State of Arrival|\n",
    "|   **depdate**     |   Departure Date from US |\n",
    "|   **i94bir**      |   Age of Respondent in Years |\n",
    "|   **i94visa**     |   Visa codes collapsed into three categories (1 = Business, 2 = Pleasure, 3 = Student)|\n",
    "|   **count**       |   Used for Summary Statistics |\n",
    "|   **dtadfile**    |   Character Date Field - Data of Arrival |\n",
    "|   **visapost**    |   Department of State where where Visa was issued |\n",
    "|   **occup**       |   Occupation that will be performed in U.S. |\n",
    "|   **entdepa**     |   Arrival Flag - admitted or paroled into the U.S. |\n",
    "|   **entdepd**     |   Departure Flag - Departed, lost I-94 or is deceased |\n",
    "|   **entdepu**     |   Update Flag - Either apprehended, overstayed, adjusted to perm residence |\n",
    "|   **matflag**     |   Match flag - Match of arrival and departure records |\n",
    "|   **biryear**     |   4 digit year of birth |\n",
    "|   **dtaddto**     |   Character Date Field - Date until which allowed to stay in U.S. |\n",
    "|   **gender**      |   Non-immigrant sex |\n",
    "|   **insnum**      |   INS number |\n",
    "|   **airline**     |   Airline used to arrive in U.S. |\n",
    "|   **admnum**      |   Admission Number |\n",
    "|   **fltno**       |    Flight number of Airline used to arrive in U.S. |\n",
    "|   **visatype**    |   Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "*Assumptions are made where the column description was not clear enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Temperature Data\n",
    "\n",
    "This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data). The raw data comes from the [Berkeley Earth data page.](http://berkeleyearth.org/archive/data/)\n",
    "\n",
    "For the capstone project, we will be using the file from the workspace \"../../data2/GlobalLandTemperaturesByCity.csv\".\n",
    "The data set provide city temperatures from year 1743 to 2013. But the immigration data is from year 2016. So we can't directly use this as table, instead we can aggregate the city info to find the average temperature on a country-level and create a new dimension table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = pd.read_csv(temperature_fname)\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking which columns tend to be empty\n",
    "df_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **dt**       |  Date in 'YYYY-MM-DD' format |\n",
    "|   **AverageTemperature**       |   Average temperature in degrees |\n",
    "|   **AverageTemperatureUncertainty**      |   Average temperature uncertainity in degrees |\n",
    "|   **City**      |   City name |\n",
    "|   **Country**      |   Country name |\n",
    "|   **Latitude**      |   Latitude |\n",
    "|   **Longitude**      |   Longitude |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature_by_country = df_temperature.groupby([\"Country\"]).agg({\"AverageTemperature\": \"mean\", \n",
    "                                                                        \"Latitude\": \"first\", \"Longitude\": \"first\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature_by_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. U.S. Cities Deographic Data\n",
    "\n",
    "This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "For the capstone project, we will be using the file from the workspace **\"us-cities-demographics.csv\".**\n",
    "The data set contains city demographics. In order to use it with our immigration fact table, we can aggregate certains columns to get the state-level info. This could be our new dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_cities = pd.read_csv(\"us-cities-demographics.csv\", delimiter=\";\")\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking which columns tend to be empty\n",
    "df_cities.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **City**      |   City name |\n",
    "|   **State**         |   U.S. State of city |\n",
    "|   **Median Age**      |   Median of the age of population |\n",
    "|   **Male Population** |   Size of male population |\n",
    "|   **Female Population**      |   Size of female population |\n",
    "|   **Total Population**      |   Size of total population |\n",
    "|   **Number of Veterans**      |   Number of Veterans in city |\n",
    "|   **Foreign-born**      |   Number of foreign-borns in city |\n",
    "|   **Average Household Size**      |   Average size of house-hold |\n",
    "|   **State Code**      |   Code of the state of city |\n",
    "|   **Race**      |   Majority race in the city |\n",
    "|   **Count**      |   Population of the majority race |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. Airport Data\n",
    "\n",
    "This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code (from wikipedia).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **ident**      |   Unique identifier |\n",
    "|   **type**         |   Type of the airport |\n",
    "|   **name**      |   Name of the airport |\n",
    "|   **elevation_ft** |   Altitude of the airport |\n",
    "|   **continent**      |   Continent of the airport |\n",
    "|   **iso_country**      |   ISO Code of the country of the airport |\n",
    "|   **iso_region**      |   ISO Code of the region of the airport |\n",
    "|   **municipality**      |   City of the airport |\n",
    "|   **gps_code**      |   GPS Code of the airport |\n",
    "|   **iata_code**      |   IATA Code of the airport |\n",
    "|   **local_code**      |   Local Code of the airport |\n",
    "|   **coordinates**      |   GPS coordinates of the airport |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For the capstone project, we will not use this data set, because the available keys don't seem to match with the main fact table with immigration data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5. I94 Labels\n",
    "\n",
    "There is a i94 Label descriptions file **I94_SAS_Labels_Descriptions.SAS_** provided in the workspace as well. For captone project, I have extracted the label descriptions from this file and put the labels in corresponding csvs, placed in i94_labels folder, which can later be used as a lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Following diagram shows the Star Scema Data Model used for Capstone Project.\n",
    "With **Immigration** Table at the center as a Fact Table. **Country_Temperature** and **State_Demographics** tables as Dimension Table.\n",
    "\n",
    "![](./images/Data_Model.jpg)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Some python functions were written using PySpark to perform initial ETL (Extract, Transform and Load) processing of the data sets and store them as parquet files.\n",
    "These functions can be found in folder **etl** folder.\n",
    "Following are the steps to perform ETL using those Python functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading the functions\n",
    "from etl.etl import create_spark_session, etl_immigration, etl_temperature_by_country, etl_state_demographics\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Creating the Spark Session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### ETL processing for 'Immigration' Table\n",
    "In the step we perform the following tasks:\n",
    "- Extract the immigration data from *'sas7bdat'* file format.\n",
    "- Transform the selected columns with appropriate data types.\n",
    "- Load it on AWS S3 Bucket in the parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform ETL processing for Immigration Fact Table for April 2016\n",
    "immigration_table = etl_immigration(spark, \n",
    "                input_data=\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\", \n",
    "                output_path=\"parquet-output/immigration\")\n",
    "                #output_path=\"s3a://zafarsohaib/immigration\")\n",
    "immigration_table.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for NULL values \n",
    "immigration_table.select([count(when(isnull(c), c)).alias(c) for c in immigration_table.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### ETL Processing for 'Country_Temperature' Table\n",
    "In the step we perform the following tasks:\n",
    "\n",
    "- Loads of the csv file of the global temperature and I94CIT_I94RES labels\n",
    "- Aggregates the temperatures dataset by country and rename new columns\n",
    "- Join the two datasets based on Country Name\n",
    "- Save the resulting parquet file to the staging area in Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform ETL processing for Country_Temperature Dimension Table\n",
    "country_temperature_table = etl_temperature_by_country(spark, \n",
    "                          input_data=\"../../data2/GlobalLandTemperaturesByCity.csv\", \n",
    "                          label_data=\"i94_labels/I94CIT_I94RES.csv\",\n",
    "                          output_path=\"parquet-output/country_temperature\")\n",
    "                          #output_path=\"s3a://zafarsohaib/country_temperature\")\n",
    "                                                       \n",
    "country_temperature_table.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for NULL values \n",
    "country_temperature_table.select([count(when(isnull(c), c)).alias(c) for c in country_temperature_table.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading from parquet to verfify schema\n",
    "#par = spark.read.parquet(\"s3a://zafarsohaib/state_demographics\")\n",
    "par = spark.read.parquet(\"parquet-output/state_demographics\")\n",
    "par.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### ETL Processing for 'State_Demographics' Table\n",
    "In the step we perform the following tasks:\n",
    "\n",
    "- Loads the csv file of the state demographics and I94ADDR labels.\n",
    "- Aggregates the demographics dataset by state and rename new columns.\n",
    "- Join the two datasets based on State Name.\n",
    "- Save the resulting parquet file to the staging area in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform ETL processing for State_Demographics Dimension Table\n",
    "state_demographics_table = etl_state_demographics(spark, \n",
    "                       input_data=\"us-cities-demographics.csv\", \n",
    "                       label_data=\"i94_labels/I94ADDR.csv\",\n",
    "                       output_path=\"parquet-output/state_demographics\")\n",
    "                       #output_path=\"s3a://zafarsohaib/state_demographics\")\n",
    "state_demographics_table.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for NULL values \n",
    "state_demographics_table.select([count(when(isnull(c), c)).alias(c) for c in state_demographics_table.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "The Data Pipeline can be divided into two phases. First phase perform ETL processes using Spark on AWS EMR Cluster, and stages the resulting parquet files in AWS S3 Buckets. The second phase involves using Apache Airflow pipelining to trigger loading of files from S3 buckets to AWS Redshift, and finally the tables are checked to ensure data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![](./images/Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airflow DAG\n",
    "\n",
    "First step after the execution has started, is to create table in Redshift (if doesn't exist), the SQL queries to create tables are stored in the file **\"/airflow/dags/create_tables.sql\"**. (*NOTE: As our Dimension Tables are not too big we chose an ALL distribution style, to speed up our analytical queries for joins*). After than we load the fact and dimension tables by loading the parquet files stored on AWS S3, in the last step we run the data quality checks as explained in next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![](./images/DAG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "After all the tables are loaded to Radshift dataware house. The last step of the Airflow Pipeline, called \"Run_data_quality_checks\", verifies if the tables record count is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Below you can find a data dictionary for the data model. For each field, there is a brief description of what the data is and where it came from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact Table: Immigration \n",
    "\n",
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **ID**      |   Primary Key (renamed from: cicid) |\n",
    "|   **YEAR**         |   4-digit Year (renamed from: i94yr) |\n",
    "|   **MONTH**      |  Month Number (renamed from: i94mon)  |\n",
    "|   **ARR_DATE** |   Arrival Data 'MM-DD-YYYY' (renamed from: arrdate)  |\n",
    "|   **DEP_DATE**      |   Departure Data 'MM-DD-YYYY' |\n",
    "|   **ARR_CITYCODE**      |   Arrival City Code |\n",
    "|   **ARR_STATECODE**      |   Arrival State Code |\n",
    "|   **ARR_MODE**      |   Arrival Transport Mode |\n",
    "|   **ARR_FLIGHT**      |   Arrival Flight Number |\n",
    "|   **ARR_AIRLINE**      |   Arrival Airline |\n",
    "|   **RES_BIRTHYEAR**      |   Respondent's Birth Year |\n",
    "|   **RES_COUNTRYCODE**      |   Respondent's Residence Country Code |\n",
    "|   **RES_GENDER**      |   Respondent's Gender |\n",
    "|   **VISA_EXPIRYDATE**      |   VISA Expiry Date |\n",
    "|   **VISA_ISSUESTATE**      |   VISA Issue State |\n",
    "|   **VISA_CATEGORY**      |   VISA Category |\n",
    "|   **VISA_TYPE**      |   VISA Type |\n",
    "\n",
    "#### Dimension Table: Country_Temperature \n",
    "\n",
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **Code**      |   Country Code (Foreign Key) |\n",
    "|   **Country**         |   Country Name |\n",
    "|   **AverageTemperature**      |  Average Temperature in Celsius  |\n",
    "|   **Latitude** |   GPS Latitude   |\n",
    "|   **Longitude**      |   GPS Longitude |\n",
    "\n",
    "\n",
    "#### Dimension Table: State_Demographics \n",
    "\n",
    "|  Column Name | Description  |\n",
    "|---|---|\n",
    "|   **Code**      |   State Code (Foreign Key) |\n",
    "|   **State**         | State Name   |\n",
    "|   **Male_Population**      |  Number of Males in State  |\n",
    "|   **Female_Population** |   Number of Females in State |\n",
    "|   **Total_Population**      |   Total population of State |\n",
    "|   **Veterans**      |   Number of Veterans in State |\n",
    "|   **Foreign_born**      |   Number of Foreign_born in State |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "***Clearly state the rationale for the choice of tools and technologies for the project.***\n",
    "\n",
    "The whole Capstone project is implemented to be deployed on AWS Cloud. The AWS Cloud Services were chosen because of following benefits:\n",
    "\n",
    "1. **Trade capital expense for variable expense –** Instead of having to invest heavily in data centers and servers before you know how you’re going to use them, you can pay only when you consume computing resources, and pay only for how much you consume.\n",
    "\n",
    "2. **Benefit from massive economies of scale –** By using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers is aggregated in the cloud, providers such as AWS can achieve higher economies of scale, which translates into lower pay as-you-go prices.\n",
    "\n",
    "3. **Stop guessing capacity –** Eliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often end up either sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little capacity as you need, and scale up and down as required with only a few minutes’ notice.\n",
    "\n",
    "4. **Increase speed and agility –** In a cloud computing environment, new IT resources are only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.\n",
    "\n",
    "5. **Stop spending money running and maintaining data centers –** Focus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking, and powering servers.\n",
    "\n",
    "6. **Go global in minutes –** Easily deploy your application in multiple regions around the world with just a few clicks. This means you can provide lower latency and a better experience for your customers at minimal cost.\n",
    "\n",
    "In particular, we are using the following AWS Services:\n",
    "\n",
    "**Amazon S3** It has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites.\n",
    "\n",
    "**Amazon Elastic MapReduce (EMR)** is an Amazon Web Services (AWS) tool for big data processing and analysis. Amazon EMR offers the expandable low-configuration service as an easier alternative to running in-house cluster computing.\n",
    "\n",
    "**Redshift** is very fast when it comes to loading data and querying it for analytical and reporting purposes. Redshift has Massively Parallel Processing (MPP) Architecture which allows you to load data at blazing fast speed. In addition, using this architecture, Redshift distributes and parallelize your queries across multiple nodes. It is horizontally scalable, fully-managed datawarehouse and offers attractive and transparent 'pay-as-you-go' pricing.\n",
    "\n",
    "**Apache Spark** is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Spark provides API for Python called PySpark.\n",
    "\n",
    "***Propose how often the data should be updated and why.***\n",
    "\n",
    "The data would be updated on a monthly basis as we recieve one file per month.\n",
    "\n",
    "***Write a description of how you would approach the problem differently under the following scenarios:***\n",
    "\n",
    "***- The data was increased by 100x.***\n",
    "\n",
    "As our project is deployed in Cloud, scalability will not be an issue. We can increase the number of nodes for EMR Cluster. We can also resize Redshift Cluster to more storage-optimized xlarge node types.\n",
    "\n",
    "***- The data populates a dashboard that must be updated on a daily basis by 7am every day.***\n",
    "\n",
    "The Airflow DAG can be scheduled to meet this demoand.\n",
    "\n",
    "***- The database needed to be accessed by 100+ people.***\n",
    "\n",
    "Amazon Redshift Cluster can be extended with more compute nodes. There is also an option of \"Elastic Resize\" available. Elastic resize significantly improves your ability to scale your Amazon Redshift clusters on-demand. Together with features such as Amazon Redshift Spectrum, it enables you to independently scale storage and compute so that you can adapt to the evolving analytical needs of your business."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
